# Generated by Django 3.2.16 on 2022-10-24 13:39

import json

from django.db import migrations, transaction
from django.utils.timezone import now


BATCH_SIZE = 2500


def migrate_change_add(apps, schema_editor):
    Change = apps.get_model('serverdb', 'Change')
    ChangeAdd = apps.get_model('serverdb', 'ChangeAdd')

    print('')  # Print on next line of migration output
    print('\t- Starting migrating ChangeAdd objects.')

    total = ChangeAdd.objects.count()
    migrated = 0
    while ChangeAdd.objects.count() > 0:
        start_time = now()

        with transaction.atomic():
            changes = ChangeAdd.objects.all()[:BATCH_SIZE]
            Change.objects.bulk_create([
                Change(
                    object_id=change.server_id,
                    change_type='create',
                    change_json=json.loads(change.attributes_json),
                    commit=change.commit,
                )
                for change in changes
            ])
            ChangeAdd.objects.filter(pk__in=changes.values_list('pk')).delete()

        batch_size = len(changes)
        migrated += batch_size
        print_progress(migrated, total, batch_size, now() - start_time)

    print('\t- Finished migrating ChangeAdd objects.')


def migrate_change_delete(apps, schema_editor):
    Change = apps.get_model('serverdb', 'Change')
    ChangeDelete = apps.get_model('serverdb', 'ChangeDelete')

    print('\t- Starting migrating ChangeDelete objects.')

    total = ChangeDelete.objects.count()
    migrated = 0
    while ChangeDelete.objects.count() > 0:
        start_time = now()

        with transaction.atomic():
            changes = ChangeDelete.objects.all()[:BATCH_SIZE]
            Change.objects.bulk_create([
                Change(
                    object_id=change.server_id,
                    change_type='delete',
                    change_json=json.loads(change.attributes_json),
                    commit=change.commit,
                )
                for change in changes
            ])
            ChangeDelete.objects.filter(
                pk__in=changes.values_list('pk')).delete()

        batch_size = len(changes)
        migrated += batch_size
        print_progress(migrated, total, batch_size, now() - start_time)

    print('\t- Finished migrating ChangeDelete objects.')


def migrate_change_update(apps, schema_editor):
    Change = apps.get_model('serverdb', 'Change')
    ChangeUpdate = apps.get_model('serverdb', 'ChangeUpdate')

    print('\t- Starting migrating ChangeUpdate objects.')

    total = ChangeUpdate.objects.count()
    migrated = 0
    while ChangeUpdate.objects.count() > 0:
        start_time = now()

        with transaction.atomic():
            changes = ChangeUpdate.objects.all()[:BATCH_SIZE]
            Change.objects.bulk_create([
                Change(
                    object_id=change.server_id,
                    change_type='change',
                    change_json=json.loads(change.updates_json),
                    commit=change.commit,
                )
                for change in changes
            ])
            ChangeUpdate.objects.filter(
                pk__in=changes.values_list('pk')).delete()

        batch_size = len(changes)
        migrated += batch_size
        print_progress(migrated, total, batch_size, now() - start_time)

    print('Finished migrating ChangeUpdate objects.')


def print_progress(current, total, batch_size, batch_duration):
    percentage_done = current / total * 100
    rate = batch_size / batch_duration.total_seconds()
    estimated_minutes_left = (total - current) / rate

    print(
        f'\t- [{now()}] Migrated {current}/{total} | {percentage_done:.2f}%, '
        f'{rate:.2f} objects/second, '
        f'circa {estimated_minutes_left:.2f} minutes left.')


class Migration(migrations.Migration):
    atomic = False

    dependencies = [
        ('serverdb', '0011_create_change_table'),
    ]

    # The migration of the old table is in a dedicated migration to avoid
    # running them together with the DDL statements which cause a full table
    # lock.
    #
    # The migration is developed to be robust and run in the background not
    # speed. You can abort <CTRL>+<C> the migration and run it again until
    # the migration is finished. For instances with a small history this is
    # probably not necessary but for instances with millions of rows this
    # makes sense.
    operations = [
        migrations.RunPython(migrate_change_add),
        migrations.RunPython(migrate_change_delete),
        migrations.RunPython(migrate_change_update),
    ]
